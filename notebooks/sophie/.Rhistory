mean_length <- mean(d$length, na.rm=TRUE)
## Like average rating, length has a lot of 0's.
qqnorm((d$length))
hist((d$length))
hist(log(d$length))
qqnorm(log(d$length))
qqnorm((d$length))
summary(d$length)
par(mfrow=c(1,2))
hist(log(d$length))
qqnorm(log(d$length))
par(mfrow=c(1,1))
mean_length <- mean(d$length, na.rm=TRUE)
par(mfrow=c(1,2))
hist(d$average_rating)
qqnorm(d$average_rating)
par(mfrow=c(1,1))
mean_rate <- mean(d$average_rating, na.rm=TRUE)
d.cor <- cor(select(d,
length, log(views),average_rating),
use = "pairwise.complete.obs")
d1 <- d
# remove NA's
d1 <- d1[!is.na(d1$views),]
# remove 11+ minute videos
d1 <- d1[d1$length <= 660,]
# assign mean rating to unrated videos
d1[d1$rate == 0, "rate"] <- mean_rate #3.744
# assign mean rating to unrated videos
d1[d1$rate == 0, "rate"] <- mean_rate #3.744
# assign mean rating to unrated videos
d1[d1$average_rating == 0, "rate"] <- mean_rate #3.744
d1.cor <- cor(cbind(log(d$views), d$views, d$average_rating, d$length, deparse.level=2))
d.cor
corrplot(d.cor,
method="color",
type="upper",
addCoef.col = "black")
d1.cor
corrplot(d1.cor,
method="color",
type="upper",
addCoef.col = "black")
d1.cor
cor(cbind(log(d$views), d$views, d$average_rating, d$length, deparse.level=2))
d1.cor <- cor(cbind(log(d1$views), d1$views, d1$average_rating, d1$length, deparse.level=2))
d1.cor
corrplot(d1.cor,
method="color",
type="upper",
addCoef.col = "black")
dim(d1)
model <- lm(views ~ rate + length, data=d1)
stargazer(
model,
type = 'text',
se = list(get_robust_se(model))
)
model <- lm(log(views) ~ rate + length, data=d1)
stargazer(
model,
type = 'text',
se = list(get_robust_se(model))
)
model <- lm(log(views) ~ average_ratings + length, data=d1)
model <- lm(log(views) ~ average_rating + length, data=d1)
stargazer(
model,
type = 'text',
se = list(get_robust_se(model))
)
scatterplotMatrix(~log(d1$views), d1$views, d1$average_rating, d1$length,
data = NULL, plot.points = F)
d1.sub <- cbind(log(d1$views), d1$views, d1$average_rating, d1$length)
d1.cor <- cor(d1.sub, deparse.level=2)
d1.cor <- cor(d1.sub)
d1.sub <- cbind(log(d1$views), d1$views, d1$average_rating, d1$length, deparse.level=2))
d1.sub <- cbind(log(d1$views), d1$views, d1$average_rating, d1$length, deparse.level=2)
# looking at the correlation matrix, the log(views) has a relatively strong correlation with average ratings, but is weakly correlated with length.
# it won't explain a huge amount of variation in the videos despite some interaction effect. The explanatory power is weak.
pair(d1.sub)
# looking at the correlation matrix, the log(views) has a relatively strong correlation with average ratings, but is weakly correlated with length.
# it won't explain a huge amount of variation in the videos despite some interaction effect. The explanatory power is weak.
pairs(d1.sub)
scatterplotMatrix(~log(d1$views), d1$views, d1$average_rating, d1$length,
data = NULL, plot.points = F)
scatterplotMatrix(~log(d1$views)+ d1$views+ d1$average_rating+ d1$length,
data = NULL, plot.points = F)
# looking at the correlation matrix, the log(views) has a relatively strong correlation with average ratings, but is weakly correlated with length.
# it won't explain a huge amount of variation in the videos despite some interaction effect. The explanatory power is weak.
pairs(d1.sub)
scatterplotMatrix(~log(d1$views)+ d1$views+ d1$average_rating+ d1$length,
data = NULL, plot.points = F)
plot(model, which=1)
plot(model, which=1)
plot(model, which=2)
plot(model, which=3)
plot(model, which=4)
plot(model, which=5)
coftest(model1, vcov=vcovHC)
coeftest(model1, vcov=vcovHC)
shapiro.test(sample(model$residuals, size=5000, replace=TRUE))
vif(model1) # has to be identical
## remove this commented block and write code that can help you assess whether
## your model satisfied the requirement of homoskedastic errors
bptest(model)
VIF(model1) # has to be identical
install.packages("VIF")
install.packages("VIF")
library(VIF)
vif(model1) # has to be identical
vif(model) # has to be identical
model <- lm(log(views) ~ average_rating + length, data=d1)
vif(model) # has to be identical
install.packages("car")
install.packages("car")
library(car)
vif(model) # has to be identical
# no strong correlation between the two
coeftest(model)
stargazer(
model,
type = 'text',
se = list(get_robust_se(model))
)
dim(d) # 9618   10
sum(d$video_id == "#NAME?") # 129
sum(nchar(as.character(d$video_id)) != 11) # 129
sum(is.na(d$views)) # 9
dim(d) # 9618   10
sum(d$video_id == "#NAME?") # 129
sum(nchar(as.character(d$video_id)) != 11) # 129
sum(is.na(d$views)) # 9
> "The dataset contains 9618 rows and 9 features (excluding log_average_ratings), which makes it a large sample. In terms of errors and NA's, there are 129 rows with name errors and 9 NAs for the video_id."
ggplot(data = d) +
geom_histogram(aes(x = log(views))) +
labs(
x = "ln(Views)",
title = "Distribution of ln(Views)"
)
summary(log(d$views), na.rm=T)
# mean = 7.286, median = 7.281.
# mean and median are similar.
ggplot(data = d) +
geom_histogram(aes(x = log(views)), bins=30) +
labs(
x = "ln(Views)",
title = "Distribution of ln(Views)"
)
ggplot(data = d) +
geom_histogram(aes(x = log(views)), bins=30) +
labs(
x = "ln(Views)",
title = "Distribution of ln(Views)"
)
summary(log(d$views), na.rm=T)
# mean = 7.286, median = 7.281.
# mean and median are similar.
par(mfrow=c(1,2))
qqnorm(d$views)
qqnorm(log(d$views))
par(mfrow=c(1,1))
## qqnorm plot of log(views) aligns along a line, confirming that this is normally distributed.
dim(d) # 9618   10
sum(d$video_id == "#NAME?") # 129
sum(nchar(as.character(d$video_id)) != 11) # 129
sum(is.na(d$views)) # 9
ggplot(data = d) +
geom_histogram(aes(x = log(views)), bins=30) +
labs(
x = "ln(Views)",
title = "Distribution of ln(Views)"
)
summary(log(d$views), na.rm=T)
hist(d$views)
qqnorm(d$views)
hist(d$views)
qqnorm(d$views)
qqnorm(log(d$views))
par(mfrow=c(1,1))
par(mfrow=c(1,2))
qqnorm(d$views)
qqnorm(log(d$views))
par(mfrow=c(1,1))
par(mfrow=c(1,2))
hist(d$average_rating)
qqnorm(d$average_rating)
par(mfrow=c(1,1))
mean_rate <- mean(d$average_rating, na.rm=TRUE)
par(mfrow=c(1,2))
hist(log(d$length))
qqnorm(log(d$length))
par(mfrow=c(1,1))
mean_length <- mean(d$length, na.rm=TRUE)
library(tidyverse)
library(ggplot2)
library(sandwich)
library(stargazer)
knitr::opts_chunk$set(echo = TRUE)
source('./src/load_and_clean.R')
source('./src/get_robust_se.R')
d <- load_and_clean(input = 'videos.txt')
glimpse(d)
dim(d) # 9618   10
sum(d$video_id == "#NAME?") # 129
sum(nchar(as.character(d$video_id)) != 11) # 129
sum(is.na(d$views)) # 9
ggplot(data = d) +
geom_histogram(aes(x = log(views)), bins=30) +
labs(
x = "ln(Views)",
title = "Distribution of ln(Views)"
)
summary(log(d$views), na.rm=T)
par(mfrow=c(1,2))
qqnorm(d$views)
qqnorm(log(d$views))
par(mfrow=c(1,1))
par(mfrow=c(1,2))
hist(d$average_rating)
qqnorm(d$average_rating)
par(mfrow=c(1,1))
mean_rate <- mean(d$average_rating, na.rm=TRUE)
par(mfrow=c(1,2))
hist(log(d$length))
qqnorm(log(d$length))
par(mfrow=c(1,1))
mean_length <- mean(d$length, na.rm=TRUE)
par(mfrow=c(1,2))
hist(log(d$length))
qqnorm((d$length))
par(mfrow=c(1,1))
mean_length <- mean(d$length, na.rm=TRUE)
d1 <- d
# remove NA's
d1 <- d1[!is.na(d1$views),]
# remove 11+ minute videos
d1 <- d1[d1$length <= 660,]
# assign mean rating to unrated videos
d1[d1$average_rating == 0, "rate"] <- mean_rate #3.744
dim(d1) # 9558   11
# After cleaning the rows, we lost 60 samples.
d1 <- d
# remove NA's
d1 <- d1[!is.na(d1$views),]
# remove 11+ minute videos
d1 <- d1[d1$length <= 660,]
# assign mean rating to unrated videos
d1[d1$average_rating == 0, "rate"] <- mean_rate #3.744
dim(d1) # 9558   11
# After cleaning the rows, we lost 60 samples.
# plot correlation matrix of covariates we are investigating.
d1.sub <- cbind(log(d1$views), d1$views, d1$average_rating, d1$length, deparse.level=2)
d1.cor <- cor(d1.sub)
corrplot(d1.cor,
method="color",
type="upper",
addCoef.col = "black")
pairs(d1.sub)
scatterplotMatrix(~log(d1$views)+ d1$views+ d1$average_rating+ d1$length,
data = NULL, plot.points = F)
model <- lm(log(views) ~ average_rating + length, data=d1)
stargazer(
model,
type = 'text',
se = list(get_robust_se(model))
)
library(tidyverse)
library(ggplot2)
library(sandwich)
library(stargazer)
library(corrplot)
knitr::opts_chunk$set(echo = TRUE)
source('./src/load_and_clean.R')
source('./src/get_robust_se.R')
d <- load_and_clean(input = 'videos.txt')
glimpse(d)
dim(d) # 9618   10
sum(d$video_id == "#NAME?") # 129
sum(nchar(as.character(d$video_id)) != 11) # 129
sum(is.na(d$views)) # 9
ggplot(data = d) +
geom_histogram(aes(x = log(views)), bins=30) +
labs(
x = "ln(Views)",
title = "Distribution of ln(Views)"
)
summary(log(d$views), na.rm=T)
par(mfrow=c(1,2))
qqnorm(d$views)
qqnorm(log(d$views))
par(mfrow=c(1,1))
par(mfrow=c(1,2))
hist(d$average_rating)
qqnorm(d$average_rating)
par(mfrow=c(1,1))
mean_rate <- mean(d$average_rating, na.rm=TRUE)
par(mfrow=c(1,2))
hist(log(d$length))
qqnorm(log(d$length))
par(mfrow=c(1,1))
mean_length <- mean(d$length, na.rm=TRUE)
d1 <- d
# remove NA's
d1 <- d1[!is.na(d1$views),]
# remove 11+ minute videos
d1 <- d1[d1$length <= 660,]
# assign mean rating to unrated videos
d1[d1$average_rating == 0, "rate"] <- mean_rate #3.744
dim(d1) # 9558   11
# After cleaning the rows, we lost 60 samples.
# plot correlation matrix of covariates we are investigating.
d1.sub <- cbind(log(d1$views), d1$views, d1$average_rating, d1$length, deparse.level=2)
d1.cor <- cor(d1.sub)
corrplot(d1.cor,
method="color",
type="upper",
addCoef.col = "black")
# pairs(d1.sub)
scatterplotMatrix(~log(d1$views)+ d1$views+ d1$average_rating+ d1$length,
data = NULL, plot.points = F)
model <- lm(log(views) ~ average_rating + length, data=d1)
stargazer(
model,
type = 'text',
se = list(get_robust_se(model))
)
library(tidyverse)
library(ggplot2)
library(sandwich)
library(stargazer)
library(corrplot)
library(car)
plot(model, which=1) # residuals vs fitted values
# really dense area after 7
plot(model, which=2)
# QQ norm tails off b/c of theoretical limitation against 0. Above, there isn't a lot of observation, which produces deviation from the line.
plot(model, which=3) # linearity and zero-conditional mean.
# Dense area distinctly starting from 7 is concerning.
plot(model, which=4)
#
plot(model, which=5)
coeftest(model1, vcov=vcovHC)
plot(model, which=3) # linearity and zero-conditional mean.
## remove this commented block and write code that can help you assess whether
## your model satisfied the requirement of homoskedastic errors
bptest(model)
# large number of samples, reject null hypothesis
plot(model, which=2)
## remove this commented block and write code that can help you assess whether
## your model satisfied the requirement of normally distributed errors
shapiro.test(sample(model$residuals, size=5000, replace=TRUE))
# large number of samples, reject null hypothesis
plot(model, which=2)
## remove this commented block and write code that can help you assess whether
## your model satisfied the requirement of a linear conditional expectation.
# The linear conditional expectation (LCE) provides a best linear (or rather, affine) estimate of the conditional expectation and hence plays an important role in approximate Bayesian inference, especially the Bayes linear approach.
plot(model, which=1) # residuals vs fitted values
# really dense area after 7
plot(model, which=4)
plot(model, which=5)
# If points fall outside of Cook's Distance, then it is an influential observation. None of our samples fall outside the dashed line for Cook's Distance (it is outside the plot range) and so are not overly influential.
coeftest(model1, vcov=vcovHC)
plot(model, which=4)
plot(model, which=5)
# If points fall outside of Cook's Distance, then it is an influential observation. None of our samples fall outside the dashed line for Cook's Distance (it is outside the plot range) and so are not overly influential.
coeftest(model, vcov=vcovHC)
## remove this commented block and write code that can help you assess whether
## your model satisfied the requirement of normally distributed errors
shapiro.test(sample(model$residuals, size=5000, replace=TRUE))
plot(model, which=3) # linearity and zero-conditional mean.
# Dense area distinctly starting from 7 is concerning.
## remove this commented block and write code that can help you assess whether
## your model satisfied the requirement of a linear conditional expectation.
d_linear <- d1 %>%
mutate(model_residuals = resid(model))
plot1 <- d_linear %>%
ggplot(aes(x = average_rating, y = log(views))) +
geom_point() + geom_smooth(method = 'lm')
plot2 <- d_linear %>%
ggplot(aes(x = average_rating, y = model_residuals)) +
geom_point() + stat_smooth(se = TRUE)
ot1
plot1
plot2
plot(model, which=1) # residuals vs fitted values
plot(model, which=1) # residuals vs fitted values
plot1
plot2
plot1 <- d_linear %>%
ggplot(aes(x = length, y = log(views))) +
geom_point() + geom_smooth(method = 'lm')
plot2 <- d_linear %>%
ggplot(aes(x = length, y = model_residuals)) +
geom_point() + stat_smooth(se = TRUE)
plot1
plot2
plot1 <- d_linear %>%
ggplot(aes(x = log(average_rating), y = log(views))) +
geom_point() + geom_smooth(method = 'lm')
plot2 <- d_linear %>%
ggplot(aes(x = log(average_rating), y = model_residuals)) +
geom_point() + stat_smooth(se = TRUE)
plot(model, which=1) # residuals vs fitted values
plot1
plot2
coeftest(model, vcov=vcovHC)
coeftest(model)
na.omit(df)
df <- na.omit(df)
df
library(tidyverse)
library(magrittr)
library(ggplot2)
library(patchwork)
library(sandwich)
library(lmtest)
library(fec16)
library(stargazer)
library(nnet)
data <- read.csv('data/external/FIFA22_official_data.csv')
df <- (filter(data, Best.Position == "GK"))
df <- na.omit(df)
money_formatter <- function(x) {
x <- gsub("€", "", x)
x <- case_when(
grepl("K", x, fixed=TRUE) ~ as.numeric(gsub("K", "", x))*1e3,
grepl("M", x, fixed=TRUE) ~  as.numeric(gsub("M", "", x))*1e6,
TRUE ~ as.numeric(x)
)
return(x)
}
weight_formatter <- function(x) {
x <- case_when(
grepl("kg", x, fixed=TRUE) ~ as.numeric(gsub("kg", "", x)),
grepl("cm", x, fixed=TRUE) ~  as.numeric(gsub("cm", "", x)),
TRUE ~ as.numeric(x)
)
return(x)
}
df$Value <- money_formatter(df$Value)
df$Wage <- money_formatter(df$Wage)
df$Height <- weight_formatter(df$Height)
df$Weight <- weight_formatter(df$Weight)
table(df$Value)
table(df$Wage)
table(df$Wage)
(df$Wage)
data <- read.csv('data/external/FIFA22_official_data.csv')
data <- read.csv('data/external/FIFA22_official_data.csv')
df <- (filter(data, Best.Position == "GK"))
df <- (filter(data, Best.Position == "GK"))
df <- na.omit(df)
money_formatter <- function(x) {
x <- gsub("€", "", x)
x <- case_when(
grepl("K", x, fixed=TRUE) ~ as.numeric(gsub("K", "", x))*1e3,
grepl("M", x, fixed=TRUE) ~  as.numeric(gsub("M", "", x))*1e6,
TRUE ~ as.numeric(x)
)
return(x)
}
weight_formatter <- function(x) {
x <- case_when(
grepl("kg", x, fixed=TRUE) ~ as.numeric(gsub("kg", "", x)),
grepl("cm", x, fixed=TRUE) ~  as.numeric(gsub("cm", "", x)),
TRUE ~ as.numeric(x)
)
return(x)
}
df$Value <- money_formatter(df$Value)
df$Wage <- money_formatter(df$Wage)
df$Height <- weight_formatter(df$Height)
df$Weight <- weight_formatter(df$Weight)
(df$Wage)
glimpse(df)
library(tidyverse)
library(magrittr)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(patchwork)
library(sandwich)
library(lmtest)
library(fec16)
library(stargazer)
library(nnet)
data <- read.csv('data/external/FIFA22_official_data.csv')
df <- (filter(data, Best.Position == "GK"))
na.omit(df)
money_formatter <- function(x) {
x <- gsub("€", "", x)
x <- case_when(
grepl("K", x, fixed=TRUE) ~ as.numeric(gsub("K", "", x))*1e3,
grepl("M", x, fixed=TRUE) ~  as.numeric(gsub("M", "", x))*1e6,
TRUE ~ as.numeric(x)
)
return(x)
}
weight_formatter <- function(x) {
x <- case_when(
grepl("kg", x, fixed=TRUE) ~ as.numeric(gsub("kg", "", x)),
grepl("cm", x, fixed=TRUE) ~  as.numeric(gsub("cm", "", x)),
TRUE ~ as.numeric(x)
)
return(x)
}
df$Value <- money_formatter(df$Value)
df$Wage <- money_formatter(df$Wage)
df$Height <- weight_formatter(df$Height)
df$Weight <- weight_formatter(df$Weight)
glimpse(df)
